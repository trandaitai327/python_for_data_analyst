# 04. Machine Learning Basics

## M·ª•c Ti√™u
- Hi·ªÉu v·ªÅ machine learning c∆° b·∫£n
- S·ª≠ d·ª•ng scikit-learn
- X√¢y d·ª±ng v√† ƒë√°nh gi√° models
- √Åp d·ª•ng v√†o ph√¢n t√≠ch d·ªØ li·ªáu

## Machine Learning Cho Data Analyst

ML gi√∫p Data Analyst:
- **D·ª± ƒëo√°n**: D·ª± ƒëo√°n doanh s·ªë, churn, etc.
- **Ph√¢n lo·∫°i**: Ph√¢n lo·∫°i kh√°ch h√†ng, s·∫£n ph·∫©m
- **Clustering**: Nh√≥m kh√°ch h√†ng t∆∞∆°ng t·ª±
- **Recommendation**: G·ª£i √Ω s·∫£n ph·∫©m

## 1. Scikit-Learn C∆° B·∫£n

### C√†i ƒê·∫∑t

```bash
pip install scikit-learn
```

### Workflow C∆° B·∫£n

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import numpy as np

# 1. Load v√† prepare data
# 2. Split data (train/test)
# 3. Train model
# 4. Evaluate model
# 5. Make predictions
```

## 2. Linear Regression (ƒê√£ H·ªçc ·ªü Ph·∫ßn 4)

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# D·ªØ li·ªáu
np.random.seed(42)
X = np.random.randn(100, 1) * 10000000 + 10000000
y = X.flatten() * 0.5 + np.random.randn(100) * 1000000

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"R¬≤: {r2:.3f}")
print(f"RMSE: {rmse:,.0f}")
```

## 3. Classification (Ph√¢n Lo·∫°i)

### Logistic Regression

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# T·∫°o d·ªØ li·ªáu ph√¢n lo·∫°i
np.random.seed(42)
# Feature: spending amount
X = np.random.randn(200, 1) * 1000000 + 5000000
# Target: churn (0 = no, 1 = yes)
y = (X.flatten() > 5000000).astype(int)

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.3f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

**V√≠ d·ª• th·ª±c t·∫ø**: D·ª± ƒëo√°n churn
```python
# D·ªØ li·ªáu kh√°ch h√†ng
np.random.seed(42)
customer_data = pd.DataFrame({
    "Spending": np.random.normal(5000000, 2000000, 1000),
    "Frequency": np.random.poisson(10, 1000),
    "Last_Purchase_Days": np.random.exponential(30, 1000)
})

# Target: Churn (1 n·∫øu churn, 0 n·∫øu kh√¥ng)
customer_data["Churn"] = (
    (customer_data["Spending"] < 3000000) |
    (customer_data["Last_Purchase_Days"] > 60)
).astype(int)

# Features
X = customer_data[["Spending", "Frequency", "Last_Purchase_Days"]]
y = customer_data["Churn"]

# Split v√† train
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Churn Prediction Accuracy: {accuracy:.3f}")
print("\nFeature Importance:")
for feature, coef in zip(X.columns, model.coef_[0]):
    print(f"  {feature}: {coef:.4f}")
```

## 4. Decision Tree

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# D·ªØ li·ªáu
X = customer_data[["Spending", "Frequency", "Last_Purchase_Days"]]
y = customer_data["Churn"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train
model = DecisionTreeClassifier(max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Decision Tree Accuracy: {accuracy:.3f}")

# Feature importance
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": model.feature_importances_
}).sort_values("Importance", ascending=False)

print("\nFeature Importance:")
print(feature_importance)
```

## 5. Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

# Train
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Random Forest Accuracy: {accuracy:.3f}")

# Feature importance
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": model.feature_importances_
}).sort_values("Importance", ascending=False)

print("\nFeature Importance:")
print(feature_importance)
```

## 6. Clustering (K-Means)

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# D·ªØ li·ªáu kh√°ch h√†ng
customer_features = customer_data[["Spending", "Frequency", "Last_Purchase_Days"]]

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(customer_features)

# K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Th√™m clusters v√†o data
customer_data["Cluster"] = clusters

# Ph√¢n t√≠ch clusters
cluster_summary = customer_data.groupby("Cluster").agg({
    "Spending": ["mean", "std"],
    "Frequency": ["mean", "std"],
    "Last_Purchase_Days": ["mean", "std"]
})

print("Cluster Summary:")
print(cluster_summary)
```

**V√≠ d·ª• th·ª±c t·∫ø**: Ph√¢n kh√∫c kh√°ch h√†ng
```python
# Ph√¢n kh√∫c kh√°ch h√†ng d·ª±a tr√™n h√†nh vi
features = customer_data[["Spending", "Frequency"]]
X_scaled = scaler.fit_transform(features)

# T√¨m s·ªë clusters t·ªëi ∆∞u (Elbow method)
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Visualize
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, marker='o')
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Ch·ªçn K=3 v√† cluster
kmeans = KMeans(n_clusters=3, random_state=42)
customer_data["Segment"] = kmeans.fit_predict(X_scaled)

# Ph√¢n t√≠ch segments
segment_summary = customer_data.groupby("Segment").agg({
    "Spending": "mean",
    "Frequency": "mean",
    "Churn": "mean"
})

print("Customer Segments:")
print(segment_summary)
```

## 7. Model Evaluation

### Cross-Validation

```python
from sklearn.model_selection import cross_val_score

# Cross-validation
scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
print(f"Cross-validation scores: {scores}")
print(f"Mean accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

### ROC Curve (Cho Classification)

```python
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score

# Predict probabilities
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## 8. Feature Engineering

### Encoding Categorical Variables

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Label encoding
le = LabelEncoder()
customer_data["Region_Encoded"] = le.fit_transform(customer_data["Region"])

# One-hot encoding
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse=False)
region_encoded = ohe.fit_transform(customer_data[["Region"]])
```

### Feature Scaling

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Standard scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Min-Max scaling
minmax_scaler = MinMaxScaler()
X_minmax = minmax_scaler.fit_transform(X)
```

## 9. Case Study: D·ª± ƒêo√°n Doanh S·ªë

```python
# T·∫°o d·ªØ li·ªáu
np.random.seed(42)
sales_data = pd.DataFrame({
    "Price": np.random.randint(5000000, 20000000, 500),
    "Marketing_Spend": np.random.randint(20000000, 100000000, 500),
    "Customer_Satisfaction": np.random.uniform(3.0, 5.0, 500),
    "Season": np.random.choice(["Spring", "Summer", "Fall", "Winter"], 500)
})

# Target: Sales
sales_data["Sales"] = (
    sales_data["Price"] * 0.001 +
    sales_data["Marketing_Spend"] * 0.0001 +
    sales_data["Customer_Satisfaction"] * 10 +
    np.random.normal(0, 5, 500)
)

# Encode categorical
le = LabelEncoder()
sales_data["Season_Encoded"] = le.fit_transform(sales_data["Season"])

# Features
X = sales_data[["Price", "Marketing_Spend", "Customer_Satisfaction", "Season_Encoded"]]
y = sales_data["Sales"]

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train multiple models
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    results[name] = {"R¬≤": r2, "RMSE": rmse}
    
    print(f"\n{name}:")
    print(f"  R¬≤: {r2:.3f}")
    print(f"  RMSE: {rmse:.2f}")

# So s√°nh models
results_df = pd.DataFrame(results).T
print("\nModel Comparison:")
print(results_df)
```

## B√†i T·∫≠p Th·ª±c H√†nh

### B√†i 1: Classification Model
```python
# T·∫°o model ph√¢n lo·∫°i kh√°ch h√†ng:
# - Features: Spending, Frequency, Last_Purchase_Days
# - Target: Churn (0/1)
# - S·ª≠ d·ª•ng Logistic Regression v√† Random Forest
# - So s√°nh accuracy
```

### B√†i 2: Clustering
```python
# Ph√¢n kh√∫c kh√°ch h√†ng:
# - S·ª≠ d·ª•ng K-Means
# - T√¨m s·ªë clusters t·ªëi ∆∞u
# - Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm t·ª´ng cluster
```

### B√†i 3: Regression Model
```python
# D·ª± ƒëo√°n doanh s·ªë:
# - Features: Price, Marketing, Season
# - Target: Sales
# - S·ª≠ d·ª•ng Linear Regression v√† Random Forest
# - ƒê√°nh gi√° v·ªõi R¬≤ v√† RMSE
```

## T·ªïng K·∫øt

‚úÖ ƒê√£ h·ªçc:
- Scikit-learn basics
- Linear v√† Logistic Regression
- Decision Tree v√† Random Forest
- K-Means Clustering
- Model evaluation
- Feature engineering
- Cross-validation

**Ho√†n th√†nh t√†i li·ªáu! üéâ**

B·∫°n ƒë√£ h·ªçc xong to√†n b·ªô t√†i li·ªáu Python cho Data Analyst t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao!

