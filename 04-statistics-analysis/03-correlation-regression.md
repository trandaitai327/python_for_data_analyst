# 03. Correlation & Regression

## M·ª•c Ti√™u
- Hi·ªÉu ƒë∆∞·ª£c correlation v√† regression, khi n√†o s·ª≠ d·ª•ng
- T√≠nh correlation coefficient v√† interpret k·∫øt qu·∫£
- X√¢y d·ª±ng v√† ƒë√°nh gi√° regression models
- √Åp d·ª•ng v√†o c√°c t√¨nh hu·ªëng th·ª±c t·∫ø: d·ª± ƒëo√°n, ph√¢n t√≠ch m·ªëi quan h·ªá

## Correlation (T∆∞∆°ng Quan)

### Correlation L√† G√¨?

**Correlation (t∆∞∆°ng quan)** ƒëo l∆∞·ªùng m·ªëi quan h·ªá tuy·∫øn t√≠nh gi·ªØa 2 bi·∫øn.

**ƒê·∫∑c ƒëi·ªÉm**:
- **Range**: -1 ƒë·∫øn +1
- **+1**: T∆∞∆°ng quan d∆∞∆°ng ho√†n h·∫£o (tƒÉng c√πng nhau)
- **-1**: T∆∞∆°ng quan √¢m ho√†n h·∫£o (m·ªôt tƒÉng, m·ªôt gi·∫£m)
- **0**: Kh√¥ng c√≥ t∆∞∆°ng quan tuy·∫øn t√≠nh

**L∆∞u √Ω quan tr·ªçng**: **Correlation ‚â† Causation**
- Correlation ch·ªâ cho bi·∫øt 2 bi·∫øn c√≥ li√™n quan, kh√¥ng ph·∫£i nguy√™n nh√¢n
- V√≠ d·ª•: Ice cream sales v√† drowning deaths c√≥ correlation cao, nh∆∞ng kh√¥ng c√≥ nghƒ©a l√† ice cream g√¢y ch·∫øt ƒëu·ªëi

### Pearson Correlation Coefficient

**C√¥ng th·ª©c**:
\[
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
\]

**Interpretation**:
- **|r| > 0.7**: T∆∞∆°ng quan m·∫°nh
- **0.3 < |r| < 0.7**: T∆∞∆°ng quan trung b√¨nh
- **|r| < 0.3**: T∆∞∆°ng quan y·∫øu

```python
import pandas as pd
import numpy as np
from scipy.stats import pearsonr
import matplotlib.pyplot as plt

# D·ªØ li·ªáu: Gi√° qu·∫£ng c√°o vs Doanh s·ªë
np.random.seed(42)
ad_spend = np.random.normal(100, 20, 100)  # Tri·ªáu VND
sales = 50 + 0.8 * ad_spend + np.random.normal(0, 10, 100)  # T·ª∑ VND

df = pd.DataFrame({'ad_spend': ad_spend, 'sales': sales})

# T√≠nh correlation
correlation, p_value = pearsonr(df['ad_spend'], df['sales'])

print(f"Correlation coefficient: {correlation:.3f}")
print(f"P-value: {p_value:.4f}")

if abs(correlation) > 0.7:
    strength = "m·∫°nh"
elif abs(correlation) > 0.3:
    strength = "trung b√¨nh"
else:
    strength = "y·∫øu"

direction = "d∆∞∆°ng" if correlation > 0 else "√¢m"
print(f"\n‚Üí T∆∞∆°ng quan {strength} {direction}")

# Visualize
plt.figure(figsize=(10, 6))
plt.scatter(df['ad_spend'], df['sales'], alpha=0.6)
plt.xlabel('Chi Ph√≠ Qu·∫£ng C√°o (Tri·ªáu VND)')
plt.ylabel('Doanh S·ªë (T·ª∑ VND)')
plt.title(f'Correlation: {correlation:.3f}')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Correlation Matrix

Khi c√≥ nhi·ªÅu bi·∫øn, t√≠nh correlation matrix:

```python
# D·ªØ li·ªáu v·ªõi nhi·ªÅu bi·∫øn
np.random.seed(42)
data = {
    'ad_spend': np.random.normal(100, 20, 100),
    'sales': np.random.normal(130, 15, 100),
    'website_traffic': np.random.normal(5000, 1000, 100),
    'conversion_rate': np.random.normal(0.03, 0.01, 100)
}

df = pd.DataFrame(data)
# T·∫°o correlation v·ªõi sales
df['sales'] = 50 + 0.8 * df['ad_spend'] + 0.01 * df['website_traffic'] + np.random.normal(0, 5, 100)

# Correlation matrix
corr_matrix = df.corr()
print("=== CORRELATION MATRIX ===")
print(corr_matrix.round(3))

# Visualize correlation matrix
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()
```

## Regression (H·ªìi Quy)

### Regression L√† G√¨?

**Regression (h·ªìi quy)** d√πng ƒë·ªÉ:
- **D·ª± ƒëo√°n**: D·ª± ƒëo√°n gi√° tr·ªã c·ªßa bi·∫øn ph·ª• thu·ªôc (Y) d·ª±a tr√™n bi·∫øn ƒë·ªôc l·∫≠p (X)
- **Hi·ªÉu m·ªëi quan h·ªá**: Xem X ·∫£nh h∆∞·ªüng Y nh∆∞ th·∫ø n√†o

**Kh√°c v·ªõi Correlation**:
- Correlation: Ch·ªâ ƒëo l∆∞·ªùng m·ªëi quan h·ªá
- Regression: X√¢y d·ª±ng model ƒë·ªÉ d·ª± ƒëo√°n v√† hi·ªÉu impact

### Simple Linear Regression

**M√¥ h√¨nh**: \(y = \beta_0 + \beta_1 x + \epsilon\)

- **Œ≤‚ÇÄ (intercept)**: Gi√° tr·ªã Y khi X = 0
- **Œ≤‚ÇÅ (slope)**: Thay ƒë·ªïi Y khi X tƒÉng 1 ƒë∆°n v·ªã
- **Œµ (error)**: Ph·∫ßn d∆∞, kh√¥ng gi·∫£i th√≠ch ƒë∆∞·ª£c

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# D·ªØ li·ªáu: Price vs Sales
np.random.seed(42)
price = np.random.randint(500000, 20000000, 100)  # VND
sales = 200 - (price / 100000) + np.random.normal(0, 10, 100)  # Units

# Reshape cho sklearn
X = price.reshape(-1, 1)
y = sales

# Train model
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Metrics
r2 = r2_score(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))

print("=== LINEAR REGRESSION RESULTS ===")
print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_:.2f}")
print(f"Slope (Œ≤‚ÇÅ): {model.coef_[0]:.6f}")
print(f"R¬≤: {r2:.3f}")
print(f"RMSE: {rmse:.2f}")

# Interpretation
print(f"\n=== INTERPRETATION ===")
print(f"Khi gi√° tƒÉng 1 tri·ªáu VND, sales gi·∫£m {abs(model.coef_[0] * 1000000):.0f} units")
print(f"R¬≤ = {r2:.3f} ‚Üí Model gi·∫£i th√≠ch {r2*100:.1f}% variance")

# Visualize
plt.figure(figsize=(10, 6))
plt.scatter(price, y, alpha=0.6, label='Actual')
plt.plot(price, y_pred, color='red', linewidth=2, label='Predicted')
plt.xlabel('Price (VND)')
plt.ylabel('Sales (Units)')
plt.title(f'Linear Regression (R¬≤ = {r2:.3f})')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Multiple Linear Regression

Khi c√≥ nhi·ªÅu bi·∫øn ƒë·ªôc l·∫≠p:

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np

# D·ªØ li·ªáu: Ad spend, Price, Traffic ‚Üí Sales
np.random.seed(42)
n = 100
ad_spend = np.random.normal(100, 20, n)
price = np.random.normal(1000000, 200000, n)
traffic = np.random.normal(5000, 1000, n)

# Sales ph·ª• thu·ªôc v√†o c·∫£ 3 bi·∫øn
sales = (50 + 0.5 * ad_spend - 0.00001 * price + 0.005 * traffic + 
         np.random.normal(0, 5, n))

df = pd.DataFrame({
    'ad_spend': ad_spend,
    'price': price,
    'traffic': traffic,
    'sales': sales
})

# Prepare data
X = df[['ad_spend', 'price', 'traffic']]
y = df['sales']

# Train model
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Metrics
r2 = r2_score(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))

print("=== MULTIPLE LINEAR REGRESSION ===")
print(f"R¬≤: {r2:.3f}")
print(f"RMSE: {rmse:.2f}")

# Coefficients
coefficients = pd.DataFrame({
    'feature': X.columns,
    'coefficient': model.coef_,
    'abs_coefficient': np.abs(model.coef_)
}).sort_values('abs_coefficient', ascending=False)

print("\n=== COEFFICIENTS ===")
print(coefficients)

# Interpretation
print("\n=== INTERPRETATION ===")
for idx, row in coefficients.iterrows():
    feature = row['feature']
    coef = row['coefficient']
    if feature == 'ad_spend':
        print(f"{feature}: TƒÉng 1 tri·ªáu VND ‚Üí Sales tƒÉng {coef:.2f} t·ª∑ VND")
    elif feature == 'price':
        print(f"{feature}: TƒÉng 1 VND ‚Üí Sales thay ƒë·ªïi {coef:.8f} t·ª∑ VND")
    elif feature == 'traffic':
        print(f"{feature}: TƒÉng 1 visitor ‚Üí Sales tƒÉng {coef:.4f} t·ª∑ VND")
```

### Model Evaluation

#### R¬≤ (R-Squared)

**R¬≤** ƒëo l∆∞·ªùng t·ª∑ l·ªá variance ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi model:
- **R¬≤ = 1**: Model ho√†n h·∫£o
- **R¬≤ = 0**: Model kh√¥ng t·ªët h∆°n mean
- **R¬≤ < 0**: Model t·ªá h∆°n mean

**L∆∞u √Ω**: R¬≤ cao kh√¥ng c√≥ nghƒ©a l√† model t·ªët (c√≥ th·ªÉ overfitting)

#### RMSE (Root Mean Squared Error)

**RMSE** ƒëo l∆∞·ªùng sai s·ªë trung b√¨nh:
- RMSE c√†ng th·∫•p c√†ng t·ªët
- C√πng ƒë∆°n v·ªã v·ªõi bi·∫øn ph·ª• thu·ªôc

#### Residuals Analysis

Ph√¢n t√≠ch ph·∫ßn d∆∞ ƒë·ªÉ ki·ªÉm tra assumptions:

```python
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats

# T√≠nh residuals
residuals = y - y_pred

# Residuals plot
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Residuals vs Predicted
axes[0].scatter(y_pred, residuals, alpha=0.6)
axes[0].axhline(y=0, color='red', linestyle='--')
axes[0].set_xlabel('Predicted Values')
axes[0].set_ylabel('Residuals')
axes[0].set_title('Residuals vs Predicted')
axes[0].grid(True, alpha=0.3)

# Q-Q plot (ki·ªÉm tra normal distribution)
stats.probplot(residuals, dist="norm", plot=axes[1])
axes[1].set_title('Q-Q Plot (Normal Distribution Check)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Ki·ªÉm tra assumptions
print("=== RESIDUAL ANALYSIS ===")
print(f"Mean of residuals: {residuals.mean():.4f} (should be ~0)")
print(f"Std of residuals: {residuals.std():.4f}")

# Test normality
from scipy.stats import shapiro
stat, p_value = shapiro(residuals)
print(f"Shapiro-Wilk test (normality): p-value = {p_value:.4f}")
if p_value > 0.05:
    print("‚Üí Residuals c√≥ ph√¢n b·ªë normal (OK)")
else:
    print("‚Üí Residuals kh√¥ng c√≥ ph√¢n b·ªë normal (c·∫ßn xem x√©t)")
```

### Polynomial Regression

Khi m·ªëi quan h·ªá kh√¥ng tuy·∫øn t√≠nh:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import numpy as np
import matplotlib.pyplot as plt

# D·ªØ li·ªáu v·ªõi m·ªëi quan h·ªá phi tuy·∫øn
np.random.seed(42)
X = np.linspace(0, 10, 100)
y = 2 + 3*X - 0.5*X**2 + np.random.normal(0, 2, 100)

# Linear regression
X_linear = X.reshape(-1, 1)
model_linear = LinearRegression()
model_linear.fit(X_linear, y)
y_pred_linear = model_linear.predict(X_linear)
r2_linear = r2_score(y, y_pred_linear)

# Polynomial regression (degree 2)
poly_features = PolynomialFeatures(degree=2)
X_poly = poly_features.fit_transform(X_linear)
model_poly = LinearRegression()
model_poly.fit(X_poly, y)
y_pred_poly = model_poly.predict(X_poly)
r2_poly = r2_score(y, y_pred_poly)

print("=== POLYNOMIAL REGRESSION ===")
print(f"Linear R¬≤: {r2_linear:.3f}")
print(f"Polynomial R¬≤: {r2_poly:.3f}")

# Visualize
plt.figure(figsize=(12, 5))
plt.scatter(X, y, alpha=0.6, label='Actual')
plt.plot(X, y_pred_linear, color='red', linewidth=2, label=f'Linear (R¬≤={r2_linear:.3f})')
plt.plot(X, y_pred_poly, color='green', linewidth=2, label=f'Polynomial (R¬≤={r2_poly:.3f})')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Linear vs Polynomial Regression')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## Business Applications

### Case Study: D·ª± ƒêo√°n Doanh S·ªë

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split

# D·ªØ li·ªáu: Marketing spend ‚Üí Sales
np.random.seed(42)
data = {
    'month': range(1, 13),
    'ad_spend': np.random.normal(100, 20, 12),
    'email_campaigns': np.random.randint(5, 15, 12),
    'social_media_posts': np.random.randint(20, 40, 12),
    'sales': np.random.normal(150, 20, 12)
}

# T·∫°o relationship
df = pd.DataFrame(data)
df['sales'] = (50 + 0.8 * df['ad_spend'] + 2 * df['email_campaigns'] + 
               0.5 * df['social_media_posts'] + np.random.normal(0, 5, 12))

# Prepare data
X = df[['ad_spend', 'email_campaigns', 'social_media_posts']]
y = df['sales']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Metrics
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print("=== SALES PREDICTION MODEL ===")
print(f"Train R¬≤: {train_r2:.3f}")
print(f"Test R¬≤: {test_r2:.3f}")
print(f"Test RMSE: {test_rmse:.2f}")

# Coefficients
coefficients = pd.DataFrame({
    'feature': X.columns,
    'coefficient': model.coef_
})
print("\n=== COEFFICIENTS ===")
print(coefficients)

# D·ª± ƒëo√°n cho th√°ng t·ªõi
next_month = pd.DataFrame({
    'ad_spend': [120],
    'email_campaigns': [10],
    'social_media_posts': [30]
})
predicted_sales = model.predict(next_month)
print(f"\n=== PREDICTION ===")
print(f"D·ª± ƒëo√°n doanh s·ªë th√°ng t·ªõi: {predicted_sales[0]:.2f} t·ª∑ VND")
```

## Best Practices

1. **Ki·ªÉm tra assumptions**: Linear relationship, normal residuals, homoscedasticity
2. **Feature selection**: Ch·ªçn features quan tr·ªçng, tr√°nh multicollinearity
3. **Train-test split**: ƒê√°nh gi√° model tr√™n test set, kh√¥ng ch·ªâ train set
4. **Interpret coefficients**: Hi·ªÉu √Ω nghƒ©a business c·ªßa coefficients
5. **Consider non-linear**: N·∫øu linear kh√¥ng t·ªët, th·ª≠ polynomial ho·∫∑c other models

## üìù B√†i T·∫≠p Th·ª±c H√†nh

### B√†i T·∫≠p 1: Correlation Analysis
Ph√¢n t√≠ch correlation gi·ªØa c√°c bi·∫øn marketing v√† sales.

### B√†i T·∫≠p 2: Sales Prediction
X√¢y d·ª±ng regression model ƒë·ªÉ d·ª± ƒëo√°n sales d·ª±a tr√™n marketing spend.

### B√†i T·∫≠p 3: Model Evaluation
ƒê√°nh gi√° model v√† ph√¢n t√≠ch residuals ƒë·ªÉ c·∫£i thi·ªán.

---

**L∆∞u √Ω**: Regression l√† c√¥ng c·ª• m·∫°nh m·∫Ω, nh∆∞ng c·∫ßn hi·ªÉu assumptions v√† limitations. Lu√¥n validate model v√† interpret k·∫øt qu·∫£ trong business context.

